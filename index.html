
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>FreeNoise</title>
<link rel="stylesheet" href="./FreeNoise_files/css/bulma.min.css">
<link rel="stylesheet" href="./FreeNoise_files/css/bulma-carousel.min.css">
<link rel="stylesheet" href="./FreeNoise_files/css/index.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script src="./FreeNoise_files/js/bulma-carousel.min.js"></script>
<script src="./FreeNoise_files/js/index.js"></script>
<link href="./FreeNoise_files/css/style.css" rel="stylesheet">
</head>

<body data-new-gr-c-s-check-loaded="14.1117.0" data-gr-ext-installed="">
<div class="content">
  <h1><strong>FreeNoise: Tuning-Free Longer Video Diffusion <br> via Noise Rescheduling</strong></h1>
  <p id="authors">Anonymous authors<sup>*</sup><br>
    <br>
  <span style="font-size: 16px"><sup>*</sup> Paper under double-blind review
  </span></p>
    <br>
  <p style="text-align: center; font-weight: bold; font-size: 1.2em">
  ✅ totally <span style="color: red; font-weight: bold">no</span> tuning &nbsp;&nbsp;&nbsp;&nbsp;
  ✅ less than <span style="color: red; font-weight: bold">20%</span> extra time &nbsp;&nbsp;&nbsp;&nbsp;
  ✅ support <span style="color: red; font-weight: bold">512</span> frames &nbsp;&nbsp;&nbsp;&nbsp;
  </p>
</div>

<div id="carousel" class="carousel carousel-vid" style="overflow: hidden; margin-top: 0px; font-weight: bold; font-size: 1.2em">
  <div class="vid-txt">
    <p>A chihuahua in astronaut suit floating in space, cinematic lighting, glow effect</p>
    <video autoplay muted loop playsinline controls dth="1024px">
      <source src="./FreeNoise_files/1k_videos/01.mp4" type="video/mp4">
    </video>
  </div>
  <div class="vid-txt">
    <p>Campfire at night in a snowy forest with starry sky in the background</p>
    <video autoplay muted loop playsinline controls dth="1024px">
      <source src="./FreeNoise_files/1k_videos/02.mp4" type="video/mp4">
    </video>
  </div>
  <div class="vid-txt">
    <p>A corgi is swimming quickly</p>
    <video autoplay muted loop playsinline controls dth="1024px">
      <source src="./FreeNoise_files/1k_videos/05.mp4" type="video/mp4">
    </video>
  </div>
  <div class="vid-txt">
    <p>An epic tornado attacking above a glowing city at night, the tornado is made of smoke, highly detailed</p>
    <video autoplay muted loop playsinline controls dth="1024px">
      <source src="./FreeNoise_files/1k_videos/04.mp4" type="video/mp4">
    </video>
  </div>
  <div class="vid-txt">
    <p>Flying through an intense battle between pirate ships in a stormy ocean</p>
    <video autoplay muted loop playsinline controls dth="1024px">
      <source src="./FreeNoise_files/1k_videos/06.mp4" type="video/mp4">
    </video>
  </div>
  <div class="vid-txt">
    <p>A realistic photo of a horse, running, in sunset</p>
    <video autoplay muted loop playsinline controls dth="1024px">
      <source src="./FreeNoise_files/1k_videos/07.mp4" type="video/mp4">
    </video>
  </div>
  <div class="vid-txt">
    <p>A bigfoot walking in the snowstorm</p>
    <video autoplay muted loop playsinline controls dth="1024px">
      <source src="./FreeNoise_files/1k_videos/08.mp4" type="video/mp4">
    </video>
  </div>
  <div class="vid-txt">
    <p>A bunch of autumn leaves falling on a calm lake, smooth</p>
    <video autoplay muted loop playsinline controls dth="1024px">
      <source src="./FreeNoise_files/1k_videos/09.mp4" type="video/mp4">
    </video>
  </div>
</div>

<div class="content">
  <h2 style="text-align:center;"><b>Abstract</b></h2>
  <p>With the availability of large-scale video datasets and the advances of diffusion models, text-driven video generation has achieved substantial progress. However, existing video generation models are typically trained on a limited number of frames, resulting in the inability to generate high-fidelity long videos during inference. Furthermore, these models only support single-text conditions, whereas real-life scenarios often require multi-text conditions as the video content changes over time. To tackle these challenges, this study explores the potential of extending the text-driven capability to generate longer videos conditioned on multiple texts. 1) We first analyze the impact of initial noise in video diffusion models. Then building upon the observation of noise, we propose <b>FreeNoise</b>, a tuning-free and time-efficient paradigm to enhance the generative capabilities of pretrained video diffusion models while preserving content consistency. Specifically, instead of initializing noises for all frames, we reschedule a sequence of noises for long-range correlation and perform temporal attention over them by window-based function. 2) Additionally, we design a novel motion injection method to support the generation of videos conditioned on multiple text prompts. Extensive experiments validate the superiority of our paradigm in extending the generative capabilities of video diffusion models. It is noteworthy that compared with the previous best-performing method which brought about 255% extra time cost, our method incurs only negligible time cost of approximately 17%.</p>
</div>

<div class="content">
  <h2><b>Comparisons of Longer Video Generation</b></h2>
  <div style="text-align: center;">
  <video width="960" autoplay muted loop playsinline>
    <!-- Your video file here -->
    <source src="./FreeNoise_files/vid_comp_long.mp4"
    type="video/mp4">
  sorry, your browser does not support HTML5 Videos.
  </video>
  </div>
</div>

<div class="content">
  <h2><b>Comparisons of Multi-Prompt Video Generation</b></h2>
  <div style="text-align: center;">
  <video width="960" autoplay muted loop playsinline>
    <!-- Your video file here -->
    <source src="./FreeNoise_files/vid_comp_multi.mp4"
    type="video/mp4">
  sorry, your browser does not support HTML5 Videos.
  </video>
  </div>
</div>

<div class="content">
  <h2><b>Ablation for Noise Rescheduling</b></h2>
  <div style="text-align: center;">
  <video width="960" autoplay muted loop playsinline>
    <!-- Your video file here -->
    <source src="./FreeNoise_files/vid_abl_long.mp4"
    type="video/mp4">
  sorry, your browser does not support HTML5 Videos.
  </video>
  </div>
</div>


<div class="content">
  <h2><b>Ablation for Motion Injection</b></h2>
  <div style="text-align: center;">
  <video width="960" autoplay muted loop playsinline>
    <!-- Your video file here -->
    <source src="./FreeNoise_files/vid_abl_multi.mp4"
    type="video/mp4">
  sorry, your browser does not support HTML5 Videos.
  </video>
  </div>
</div>

<div class="content">
  <h2><b>Longer Results with 512 Frames</b></h2>
  <div style="text-align: center;">
  <video width="960" autoplay muted loop playsinline>
    <!-- Your video file here -->
    <source src="./FreeNoise_files/vid_long_512.mp4"
    type="video/mp4">
  sorry, your browser does not support HTML5 Videos.
  </video>
  </div>
</div>

<div class="content">
  <h2><b>Multi-Prompt Results with 256 Frames</b></h2>
  <div style="text-align: center;">
  <video width="960" autoplay muted loop playsinline>
    <!-- Your video file here -->
    <source src="./FreeNoise_files/vid_mp_256.mp4"
    type="video/mp4">
  sorry, your browser does not support HTML5 Videos.
  </video>
  </div>
</div>

<div class="content">
  <h2><b>A. Other Noise Scheduling</b></h2>
  <h5>We have explored some other strategies in our early experiments. We have tried mixed noise and progressive noise to make the fragments generated by each window more correlated [1]. However, it brings poor quality results due to the training-inference gap. In addition, we have tried to flip noise frames spatially. Although it brings more new content, abrupt changes in content are also introduced.</h5>
  <h5>[1] Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models</h5>
  <div style="text-align: center;">
  <video width="960" autoplay muted loop playsinline>
    <!-- Your video file here -->
    <source src="./FreeNoise_files/vid_comp_noise.mp4"
    type="video/mp4">
  sorry, your browser does not support HTML5 Videos.
  </video>
  </div>
</div>

<div class="content">
  <h2><b>B. Case Analysis of Significant Movement</b></h2>
  <h5>Videos with significant movement can be mainly divided into three types: (1) the lens moving with the subject, (2) the subject moving off the screen, and (3) the subject moving within the screen. These three types are automatically determined during the inference stage based on the sampled random noises and the given prompt. Since the base model (inference without FreeNoise) struggles to deal with the other two types effectively, we have only showcased instances of the lens moving with the subject for videos with significant movement in our previous results.</h5>
  <h3><b>B1. Effect of Noise Rescheduling (with Training Length 16 Frames)</b></h3>
  <h5>Noise rescheduling is able to generate new motions while maintaining the main subjects and scenes.</h5>
  <div style="text-align: center;">
  <video width="960" autoplay muted loop playsinline>
    <!-- Your video file here -->
    <source src="./FreeNoise_files/vid_shuffle_256.mp4"
    type="video/mp4">
  sorry, your browser does not support HTML5 Videos.
  </video>
  </div>
  <h3><b>B2. Real Video of Running Horse (<a href="https://youtube.com/shorts/p6VxHDn9kGU?si=L_7lQlSvaeGcSsIT" target="_blank"><b>Source</b></a>)</b></h3>
  <h5>In real videos with significant movement, the lens moving with the subject is a common case.</h5>
  <div style="text-align: center;">
  <video width="240" autoplay muted loop playsinline>
    <!-- Your video file here -->
    <source src="./FreeNoise_files/real_horse.mov"
    type="video/mp4">
  sorry, your browser does not support HTML5 Videos.
  </video>
  </div>
  <h3><b>B3. Lens Moving with the Subject</b></h3>
  <h5>For videos of this type, the position of the subject does not change much and the movement is shown through the regression of the background.</h5>
  <div style="text-align: center;">
  <video width="960" autoplay muted loop playsinline>
    <!-- Your video file here -->
    <source src="./FreeNoise_files/vid_our_256.mp4"
    type="video/mp4">
  sorry, your browser does not support HTML5 Videos.
  </video>
  </div>
  <h3><b>B4. Subject Moving off the Screen</b></h3>
  <h5>For videos of this type, the subject will move off the screen. However, the subject will suddenly appear again due to semantic constraints.</h5>
  <div style="text-align: center;">
  <video width="960" autoplay muted loop playsinline>
    <!-- Your video file here -->
    <source src="./FreeNoise_files/vid_out_256.mp4"
    type="video/mp4">
  sorry, your browser does not support HTML5 Videos.
  </video>
  </div>
  <h3><b>B5. Subject Moving within the Screen</b></h3>
  <h5>For videos of this type, the subject will move within the screen. Due to the size limitation of the screen, the subject will turn around. However, the current pretrained model behaves unnaturally when turning (even for inference without FreeNoise).</h5>
  <div style="text-align: center;">
  <video width="960" autoplay muted loop playsinline>
    <!-- Your video file here -->
    <source src="./FreeNoise_files/vid_within_256.mp4"
    type="video/mp4">
  sorry, your browser does not support HTML5 Videos.
  </video>
  </div>
  <h3><b>B6. Depth2Video</b></h3>
  <h5>FreeNoise works for ControlNet and additional condition helps to generate more diverse motions. However, naively applying FreeNoise with ControlNet can not work perfectly, because the frame-wise variated depth conditions introduce extra variation to the context. It requires further exploration to make this combination work properly.<h5>
  <div style="text-align: center;">
  <video width="960" autoplay muted loop playsinline>
    <!-- Your video file here -->
    <source src="./FreeNoise_files/vid_depth.mp4"
    type="video/mp4">
  sorry, your browser does not support HTML5 Videos.
  </video>
  </div>
</div>


<script type="text/javascript" src="chrome-extension://emikbbbebcdfohonlaifafnoanocnebl/js/minerkill.js"></script></body><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
  div.grammarly-desktop-integration {
    position: absolute;
    width: 1px;
    height: 1px;
    padding: 0;
    margin: -1px;
    overflow: hidden;
    clip: rect(0, 0, 0, 0);
    white-space: nowrap;
    border: 0;
    -moz-user-select: none;
    -webkit-user-select: none;
    -ms-user-select:none;
    user-select:none;
  }

  div.grammarly-desktop-integration:before {
    content: attr(data-content);
  }
</style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>