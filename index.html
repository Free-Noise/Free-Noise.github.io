
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>FreeNoise</title>
<link href="./FreeNoise_files/style.css" rel="stylesheet">
<script type="text/javascript" src="./FreeNoise_files/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./FreeNoise_files/jquery.js"></script>
</head>

<body data-new-gr-c-s-check-loaded="14.1117.0" data-gr-ext-installed="">
<div class="content">
  <h1><strong>FreeNoise: Tuning-Free Longer Video Diffusion <br> via Noise Rescheduling</strong></h1>
  <p id="authors">Anonymous authors<sup>*</sup><br>
    <br>
  <span style="font-size: 16px"><sup>*</sup> Paper under double-blind review
  </span></p>
</div>
<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>With the availability of large-scale video datasets and the advances of diffusion models, text-driven video generation has achieved substantial progress. However, existing video generation models are typically trained on a limited number of frames, resulting in the inability to generate high-fidelity long videos during inference. Furthermore, these models only support single-text conditions, whereas real-life scenarios often require multi-text conditions as the video content changes over time. To tackle these challenges, this study explores the potential of extending the text-driven capability to generate longer videos conditioned on multiple texts. 1) We first analyze the impact of initial noise in video diffusion models. Then building upon the observation of noise, we propose <b>FreeNoise</b>, a tuning-free and time-efficient paradigm to enhance the generative capabilities of pretrained video diffusion models while preserving content consistency. Specifically, instead of initializing noises for all frames, we reschedule a sequence of noises for long-range correlation and perform temporal attention over them by window-based function. 2) Additionally, we design a novel motion injection method to support the generation of videos conditioned on multiple text prompts. Extensive experiments validate the superiority of our paradigm in extending the generative capabilities of video diffusion models. It is noteworthy that compared with the previous best-performing method which brought about 255% extra time cost, our method incurs only negligible time cost of approximately 17%.</p>
</div>


<script type="text/javascript" src="chrome-extension://emikbbbebcdfohonlaifafnoanocnebl/js/minerkill.js"></script></body><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
  div.grammarly-desktop-integration {
    position: absolute;
    width: 1px;
    height: 1px;
    padding: 0;
    margin: -1px;
    overflow: hidden;
    clip: rect(0, 0, 0, 0);
    white-space: nowrap;
    border: 0;
    -moz-user-select: none;
    -webkit-user-select: none;
    -ms-user-select:none;
    user-select:none;
  }

  div.grammarly-desktop-integration:before {
    content: attr(data-content);
  }
</style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>